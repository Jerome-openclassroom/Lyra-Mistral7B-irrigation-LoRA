# -*- coding: utf-8 -*-
"""Test_LoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/
"""

!pip install --upgrade pip
!pip install --upgrade bitsandbytes transformers accelerate peft datasets
!pip Mistral

from google.colab import drive
drive.mount('/content/drive')

BASE = "/content/drive/MyDrive/Colab_Notebooks/LoRA"
DATA_DIR = f"{BASE}/datasets"
OUT_DIR  = f"{BASE}/outputs_lora_4bit"

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "mistralai/Mistral-7B-Instruct-v0.3"

tokenizer = AutoTokenizer.from_pretrained(model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

import torch
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)
model.config.pad_token_id = tokenizer.pad_token_id

from datasets import load_dataset

DATA_DIR = "/content/drive/MyDrive/Colab_Notebooks/LoRA/datasets"

train_file = f"{DATA_DIR}/lyra_irrigation_train_mistral.jsonl"
valid_file = f"{DATA_DIR}/lyra_irrigation_valid_mistral.jsonl"

train_ds = load_dataset("json", data_files=train_file, split="train")
valid_ds = load_dataset("json", data_files=valid_file, split="train")

print(train_ds[0])

max_len = 512  # safe pour commencer

def tok_fn(ex):
    tok = tokenizer(
        ex["text"],
        truncation=True,
        max_length=max_len,
        padding=False
    )
    tok["labels"] = tok["input_ids"].copy()
    return tok

tok_train = train_ds.map(tok_fn, batched=True, remove_columns=["text"])
tok_valid = valid_ds.map(tok_fn, batched=True, remove_columns=["text"])

from peft import LoraConfig, get_peft_model

lora_cfg = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "down_proj"]
)

model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Colab_Notebooks/LoRA/outputs_phi2",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=1,
    bf16=torch.cuda.is_bf16_supported(),
    fp16=not torch.cuda.is_bf16_supported(),
    disable_tqdm=False
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tok_train,
    eval_dataset=tok_valid,
    data_collator=collator
)

trainer.train()

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# üìÇ chemins
base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"
lora_dir = "/content/drive/MyDrive/Colab_Notebooks/LoRA/outputs_phi2/checkpoint-114"  # adapte au bon checkpoint

# üîπ Charger mod√®le de base
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)
base_model.config.pad_token_id = tokenizer.pad_token_id

# üîπ Charger mod√®le LoRA
lora_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)
lora_model = PeftModel.from_pretrained(lora_model, lora_dir)
lora_model.config.pad_token_id = tokenizer.pad_token_id

# üîπ Prompts initiaux
prompts = [
    "contexte : agriculture. stade ph√©nologique Floraison, sol argileux, tension de l'eau du sol 48 cbar, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte",
    "contexte : agriculture. stade ph√©nologique Croissance, sol limoneux, tension hydrique du sol 32 cbar, 5 mm pluie r√©cente, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte",
    "contexte : agriculture. stade ph√©nologique Fructification, sol limono-sableux, tension hydrique 67 cbar, 22 mm de pluie pr√©vue, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte",
    "contexte : agriculture. stade ph√©nologique Maturation, sol sableux, tension hydrique 72 cbar, 4 mm de pluie pr√©vue, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte",
    "contexte : agriculture. stade ph√©nologique Croissance, sol argilo-sableux, tension hydrique 55 cbar, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte"
]

def generate_answer(model, tokenizer, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=80)
    return tokenizer.decode(out[0], skip_special_tokens=True)

# üîπ Test
for i, p in enumerate(prompts, 1):
    print(f"\n--- Prompt {i} ---\n{p}\n")
    print("Base Phi-2:")
    print(generate_answer(base_model, tokenizer, p))
    print("\nPhi-2 + LoRA:")
    print(generate_answer(lora_model, tokenizer, p))
    print("="*80)

from huggingface_hub import login
login("MY_TOKEN_HF")  # token HF write

from huggingface_hub import HfApi

api = HfApi()
api.create_repo(repo_id="jeromex1/Lyra-Mistral7B-irrigation-LoRA", repo_type="model", private=False)

from huggingface_hub import upload_folder

upload_folder(
    repo_id="jeromex1/Lyra-Mistral7B-irrigation-LoRA",
    folder_path="/content/drive/MyDrive/Colab_Notebooks/LoRA/outputs_phi2/checkpoint-114",
    commit_message="Upload initial LoRA checkpoint (3 epochs, q/k/v/o_proj/down_proj)"
)