# -*- coding: utf-8 -*-
"""Test_LoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/
"""

!pip install --upgrade pip
!pip install --upgrade bitsandbytes transformers accelerate peft datasets
!pip Mistral

from google.colab import drive #on monte le disque Google Drive dans l'environnement Colab
drive.mount('/content/drive')

BASE = "/content/drive/MyDrive/Colab_Notebooks/LoRA" #d√©finit les chemins vers les datasets sous forme de chaine de caract√®res (ils sont stock√©s dans Google Drive)
DATA_DIR = f"{BASE}/datasets"
OUT_DIR  = f"{BASE}/outputs_lora_4bit"

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "mistralai/Mistral-7B-Instruct-v0.3"

tokenizer = AutoTokenizer.from_pretrained(model_id) #on s√©lectionne le Tokenizer de Mistral 7B
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

import torch # Chargement de Mistral 7B depuis Hugging Face, en version quantiz√©e 4 bits (via bitsandbytes), ce qui r√©duit la charge m√©moire n√©cessaire en gardant la majorit√© des performances du mod√®le
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto", #r√©partit les poids sur les GPU disponibles
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16 
)
model.config.pad_token_id = tokenizer.pad_token_id

from datasets import load_dataset

DATA_DIR = "/content/drive/MyDrive/Colab_Notebooks/LoRA/datasets"

train_file = f"{DATA_DIR}/lyra_irrigation_train_mistral.jsonl" #train_file est une variable string qui re√ßoit tout le chemin vers le dataset
valid_file = f"{DATA_DIR}/lyra_irrigation_valid_mistral.jsonl"

train_ds = load_dataset("json", data_files=train_file, split="train") #chargement des fichiers .jsonl contenant des paires instruction/r√©ponse
valid_ds = load_dataset("json", data_files=valid_file, split="train")

print(train_ds[0])

max_len = 512  # safe pour commencer

def tok_fn(ex):
    tok = tokenizer(
        ex["text"],
        truncation=True,
        max_length=max_len,
        padding=False #on utilise DataCollatorForLanguageModeling (voir plus bas √† la ligne 81), il ajoute le padding dynamiquement et s‚Äôassure que tous les exemples d‚Äôun batch ont la m√™me longueur, en ajoutant des pad_token_id
    )
    tok["labels"] = tok["input_ids"].copy()
    return tok

tok_train = train_ds.map(tok_fn, batched=True, remove_columns=["text"])
tok_valid = valid_ds.map(tok_fn, batched=True, remove_columns=["text"])

from peft import LoraConfig, get_peft_model
# ci dessous, la configuration du LoRA
lora_cfg = LoraConfig(
    r=16, #rang des matrices LoRA
    lora_alpha=32, #lora_alpha/r = 32/16 = 2, le facteur d'influence des matrices sur l'entrainement est de 2
    lora_dropout=0.05, #dropout de 5% √† chaque passage d'un batch 
    bias="none", #on n'entraine que sur les poids (mais sans les modifier !), pas les biais, ici 0.3% des param√®tres seulement sont entrain√©s (sur 7 milliards)
    task_type="CAUSAL_LM", # Causal signifie que Mistral 7B est un d√©codeur autoregressif : il pr√©dit le prochain Token (compl√©tion)
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "down_proj"] #on cible les t√™tes d'attention q,k et v et les couches o et down
)

model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Colab_Notebooks/LoRA/outputs_phi2", #dossier o√π on sauvegarde les checkpoints du mod√®le
    per_device_train_batch_size=1, #une seule ligne par batch (l√©ger mais ok pour un tr√®s petit dataset !)
    gradient_accumulation_steps=4, #met √† jour les poids seulement tous les 4 passages, ce qui √©quivaut √† un batch de 4
    num_train_epochs=3, #3 epochs pour l'entrainement (en moyenne entre 3 et 5)
    learning_rate=2e-5, #normal aussi pour un learning rate
    lr_scheduler_type="cosine", #pente douce au d√©part, plus forte ensuite, et douce √† la fin
    warmup_ratio=0.03, #les premiers 3% des donn√©es d'entrainement utilisent une mont√©e progressive vers le learning rate
    logging_steps=10, #on enregistre la Loss tous les 10 steps (chaque 10 cycles d'entrainement)
    save_strategy="epoch", #on enregistre un mod√®le √† la fin de chaque epoch
    save_total_limit=1, #on ne garde qu'un seul mod√®le en m√©moire
    bf16=torch.cuda.is_bf16_supported(), #binary floart si CUDA fonctionne...
    fp16=not torch.cuda.is_bf16_supported(), #...floating point 16 ordinaire si CUDA n'est pas utilis√© par le GPU
    disable_tqdm=False #on visualise la barre de progression pendant l'entrainement (indispensable pour savoir si √ßa a plant√©, o√π √ßa en est...!)
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tok_train,
    eval_dataset=tok_valid,
    data_collator=collator
)

trainer.train() #cette commande lance l'entrainement 

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# üìÇ chemins
base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"
lora_dir = "/content/drive/MyDrive/Colab_Notebooks/LoRA/outputs_phi2/checkpoint-114"  # adapte au bon checkpoint

# üîπ Charger mod√®le de base
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)
base_model.config.pad_token_id = tokenizer.pad_token_id

# üîπ Charger mod√®le LoRA
lora_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)
lora_model = PeftModel.from_pretrained(lora_model, lora_dir)
lora_model.config.pad_token_id = tokenizer.pad_token_id

# üîπ Prompts initiaux
prompts = [
    "contexte : agriculture. stade ph√©nologique Floraison, sol argileux, tension de l'eau du sol 48 cbar, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte",
    "contexte : agriculture. stade ph√©nologique Croissance, sol limoneux, tension hydrique du sol 32 cbar, 5 mm pluie r√©cente, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte",
    "contexte : agriculture. stade ph√©nologique Fructification, sol limono-sableux, tension hydrique 67 cbar, 22 mm de pluie pr√©vue, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte",
    "contexte : agriculture. stade ph√©nologique Maturation, sol sableux, tension hydrique 72 cbar, 4 mm de pluie pr√©vue, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte",
    "contexte : agriculture. stade ph√©nologique Croissance, sol argilo-sableux, tension hydrique 55 cbar, question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte"
]

def generate_answer(model, tokenizer, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=80)
    return tokenizer.decode(out[0], skip_special_tokens=True)

# üîπ Test
for i, p in enumerate(prompts, 1):
    print(f"\n--- Prompt {i} ---\n{p}\n")
    print("Base Phi-2:")
    print(generate_answer(base_model, tokenizer, p))
    print("\nPhi-2 + LoRA:")
    print(generate_answer(lora_model, tokenizer, p))
    print("="*80)

from huggingface_hub import login
login("MY_TOKEN_HF")  # token HF write

from huggingface_hub import HfApi

api = HfApi()
api.create_repo(repo_id="jeromex1/Lyra-Mistral7B-irrigation-LoRA", repo_type="model", private=False)

from huggingface_hub import upload_folder

upload_folder(
    repo_id="jeromex1/Lyra-Mistral7B-irrigation-LoRA",
    folder_path="/content/drive/MyDrive/Colab_Notebooks/LoRA/outputs_phi2/checkpoint-114",
    commit_message="Upload initial LoRA checkpoint (3 epochs, q/k/v/o_proj/down_proj)"
)
