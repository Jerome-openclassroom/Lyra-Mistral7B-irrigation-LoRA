<img src="https://upload.wikimedia.org/wikipedia/en/c/c3/Flag_of_France.svg" width="100px" height="auto" />


# üå± Lyra-Mistral7B-irrigation-LoRA

![EUstack](https://img.shields.io/badge/üá™üá∫%20EUstack-ready-blue)
![SouverainAI](https://img.shields.io/badge/üá´üá∑%20SouverainAI-oui-success)

## üìå Description
Ce d√©p√¥t d√©crit la m√©thodologie employ√©e pour r√©aliser un **fine-tuning LoRA** sur le mod√®le **Mistral-7B-Instruct-v0.3**, sp√©cialis√© pour l'irrigation agricole (r√©ponses courtes en fran√ßais donnant des apports d'eau en mm selon le sol, la tension hydrique et le stade ph√©nologique).

Le mod√®le est disponible publiquement sur Hugging Face :  
üëâ [Lyra-Mistral7B-irrigation-LoRA](https://huggingface.co/jeromex1/Lyra-Mistral7B-irrigation-LoRA)

---

## ‚öôÔ∏è M√©thodologie

### Base model
- **Nom** : `mistralai/Mistral-7B-Instruct-v0.3`
- **Param√®tres** : 7,27 milliards
- **Langue** : multilingue, excellent en fran√ßais

### Fine-tuning
- **Technique** : QLoRA (quantization 4 bits, bitsandbytes) dans Google Colab Pro avec processeur A100
- **Modules LoRA** : `q_proj`, `k_proj`, `v_proj`, `o_proj`, `down_proj`
- **Hyperparam√®tres** :
  - `r=16`
  - `lora_alpha=32`
  - `lora_dropout=0.05`
  - `batch_size=2`
  - `gradient_accumulation_steps=4`
  - `seq_length=1024`
  - `epochs=3`
- **GPU utilis√©** : A100 (40 Go)

### Param√®tres entra√Ænables
```
trainable params: 23,068,672
all params: 7,271,092,224
trainable%: 0.3173
```

---

## üìâ R√©sultats d'entra√Ænement

### Loss par steps
```
Step   Loss
10     3.3845
20     2.2109
30     1.2643
40     0.7636
50     0.5695
60     0.4991
70     0.3880
80     0.3390
90     0.3037
100    0.2860
110    0.2766
Final avg loss (epoch 3): 0.9117
```

---

## üîé Tests de prompts

### Prompt 1
```
contexte : agriculture. stade ph√©nologique Floraison, sol argileux, tension de l'eau du sol 48 cbar,
question portant sur l'irrigation : quel apport d'eau est n√©cessaire ? donner une r√©ponse courte
```
- **Base Mistral-7B** : r√©ponse vague, valeurs aberrantes (1000 m¬≥/ha).
- **LoRA** : `20 mm en goutte √† goutte`.

---

### Prompt 2
```
stade ph√©nologique Croissance, sol limoneux, tension hydrique 32 cbar, 5 mm pluie r√©cente
```
- **Base** : r√©ponse g√©n√©rique (10 mm).
- **LoRA** : `20 mm d'irrigation recommand√©e pour maintenir la tension √† 30 cbar`.

---

### Prompt 3
```
stade ph√©nologique Fructification, sol limono-sableux, tension 67 cbar, 22 mm de pluie pr√©vue
```
- **Base** : "pas n√©cessaire d'irriguer".
- **LoRA** : `10 mm d'irrigation recommand√©e pour maintenir la tension autour de 60 cbar`.

---

### Prompt 4
```
stade ph√©nologique Maturation, sol sableux, tension hydrique 72 cbar, 4 mm de pluie pr√©vue
```
- **Base** : r√©ponse hors sujet.
- **LoRA** : `2 mm suffisent pour maintenir la tension autour de 72 cbar`.

---

### Prompt 5
```
stade ph√©nologique Croissance, sol argilo-sableux, tension hydrique 55 cbar
```
- **Base** : r√©ponse incoh√©rente (27 mm/jour).
- **LoRA** : `20 mm en goutte √† goutte`.

---

## üì¶ Utilisation

Exemple de chargement du mod√®le avec PEFT :
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

base_model = "mistralai/Mistral-7B-Instruct-v0.3"
lora_model = "jeromex1/Lyra-Mistral7B-irrigation-LoRA"

tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model, load_in_4bit=True, device_map="auto")
model = PeftModel.from_pretrained(model, lora_model)

prompt = "contexte : agriculture. sol sableux, tension 70 cbar, stade Croissance, quel apport d'eau ?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50)[0], skip_special_tokens=True))
```

---

## üìú Licence
MIT
