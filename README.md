<img src="https://upload.wikimedia.org/wikipedia/en/c/c3/Flag_of_France.svg" width="100px" height="auto" />


# ğŸŒ± Lyra-Mistral7B-irrigation-LoRA

![SouverainAI](https://img.shields.io/badge/ğŸ‡«ğŸ‡·%20SouverainAI-oui-success)
![EUstack](https://img.shields.io/badge/ğŸ‡ªğŸ‡º%20EUstack-ready-blue)

## ğŸ“Œ Description
Ce dÃ©pÃ´t dÃ©crit la mÃ©thodologie employÃ©e pour rÃ©aliser un **fine-tuning LoRA** sur le modÃ¨le **Mistral-7B-Instruct-v0.3**, spÃ©cialisÃ© pour l'irrigation agricole (rÃ©ponses courtes en franÃ§ais donnant des apports d'eau en mm selon le sol, la tension hydrique et le stade phÃ©nologique).

Le modÃ¨le est disponible publiquement sur Hugging Face :  
ğŸ‘‰ [Lyra-Mistral7B-irrigation-LoRA](https://huggingface.co/jeromex1/Lyra-Mistral7B-irrigation-LoRA)

---

## âš™ï¸ MÃ©thodologie

### Base model
- **Nom** : `mistralai/Mistral-7B-Instruct-v0.3`
- **ParamÃ¨tres** : 7,27 milliards
- **Langue** : multilingue, excellent en franÃ§ais

### Fine-tuning
- **Technique** : QLoRA (quantization 4 bits, bitsandbytes) dans Google Colab Pro 
- **Modules LoRA** : `q_proj`, `k_proj`, `v_proj`, `o_proj`, `down_proj`
- **HyperparamÃ¨tres** :
  - `r=16`
  - `lora_alpha=32`
  - `lora_dropout=0.05`
  - `batch_size=2`
  - `gradient_accumulation_steps=4`
  - `seq_length=1024`
  - `epochs=3`
- **GPU utilisÃ©** : A100 (40 Go)

### ParamÃ¨tres entraÃ®nables
```
trainable params: 23,068,672
all params: 7,271,092,224
trainable%: 0.3173
```

---

## ğŸ“‰ RÃ©sultats d'entraÃ®nement

### Loss par steps
```
Step   Loss
10     3.3845
20     2.2109
30     1.2643
40     0.7636
50     0.5695
60     0.4991
70     0.3880
80     0.3390
90     0.3037
100    0.2860
110    0.2766
Final avg loss (epoch 3): 0.9117
```

---

## ğŸ” Tests de prompts

### Prompt 1
```
contexte : agriculture. stade phÃ©nologique Floraison, sol argileux, tension de l'eau du sol 48 cbar,
question portant sur l'irrigation : quel apport d'eau est nÃ©cessaire ? donner une rÃ©ponse courte
```
- **Base Mistral-7B** : rÃ©ponse vague, valeurs aberrantes (1000 mÂ³/ha).
- **LoRA** : `20 mm en goutte Ã  goutte`.

---

### Prompt 2
```
stade phÃ©nologique Croissance, sol limoneux, tension hydrique 32 cbar, 5 mm pluie rÃ©cente
```
- **Base** : rÃ©ponse gÃ©nÃ©rique (10 mm).
- **LoRA** : `20 mm d'irrigation recommandÃ©e pour maintenir la tension Ã  30 cbar`.

---

### Prompt 3
```
stade phÃ©nologique Fructification, sol limono-sableux, tension 67 cbar, 22 mm de pluie prÃ©vue
```
- **Base** : "pas nÃ©cessaire d'irriguer".
- **LoRA** : `10 mm d'irrigation recommandÃ©e pour maintenir la tension autour de 60 cbar`.

---

### Prompt 4
```
stade phÃ©nologique Maturation, sol sableux, tension hydrique 72 cbar, 4 mm de pluie prÃ©vue
```
- **Base** : rÃ©ponse hors sujet.
- **LoRA** : `2 mm suffisent pour maintenir la tension autour de 72 cbar`.

---

### Prompt 5
```
stade phÃ©nologique Croissance, sol argilo-sableux, tension hydrique 55 cbar
```
- **Base** : rÃ©ponse incohÃ©rente (27 mm/jour).
- **LoRA** : `20 mm en goutte Ã  goutte`.

---

## ğŸ“¦ Utilisation

Exemple de chargement du modÃ¨le avec PEFT :

```python
!pip install -q peft transformers accelerate bitsandbytes sentencepiece huggingface_hub hf_xet
```

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

base_model = "mistralai/Mistral-7B-Instruct-v0.3"
lora_model = "jeromex1/Lyra-Mistral7B-irrigation-LoRA"

tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model, load_in_4bit=True, device_map="auto")
model = PeftModel.from_pretrained(model, lora_model)

prompt = "contexte : agriculture. sol sableux, tension 70 cbar, stade Croissance, quel apport d'eau ?"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50)[0], skip_special_tokens=True))
```

---
## ğŸ’¾ Arborescence

```
lyra_transformer/
â”œâ”€â”€ README.md                          # version en FranÃ§ais
â”œâ”€â”€ README_fr.md                       # version en anlgais
â”œâ”€â”€ code/                              
â”‚   â”œâ”€â”€ Mistral_7B_LoRA.py             # Script de fine-tuning LoRA
â”‚
â”œâ”€â”€ datasets/                          # mes datasets au format jsonl structurÃ©s pour l'entrainement des modÃ¨les Mistral IA
â”‚   â”œâ”€â”€ lyra_irrigation_train_mistral.jsonl
â”‚   â””â”€â”€ lyra_irrigation_valid_mistral.jsonl
â”‚
â””â”€â”€ learning_curve/                    # courbe d'apprentissage
    â””â”€â”€ loss_LoRA_Mistral_7B.xlsx

```
---

Ce projet est une **variante radicalement diffÃ©rente** de [Lyra_irrigation_mobile](https://github.com/Jerome-openclassroom/Lyra_irrigation_mobile) : ici, lâ€™approche repose sur **Mistral 7B et LoRA** (fine-tuning lÃ©ger sur les poids quantifiÃ©s, avec scripts Python), tandis que lâ€™autre projet sâ€™appuie sur **GPT (API OpenAI)** avec **SFT direct sur les poids et une interface applicative**.  
ğŸ‘‰ Les deux approches, bien que techniquement opposÃ©es, aboutissent Ã  un **rÃ©sultat fonctionnellement Ã©quivalent** et mettent en valeur des compÃ©tences complÃ©mentaires.

---
## ğŸ“œ Licence
MIT
